<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '6.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Generative adversarial network (GAN) has got a lot of great results in many generative tasks to create the real-world objects which is unseen before. Inspired by Generative adversarial network (GAN) h">
<meta name="keywords" content="GAN, WGAN, WGAN-GP">
<meta property="og:type" content="article">
<meta property="og:title" content="Telling GAN">
<meta property="og:url" content="http://sanglongbest.github.io/2020/01/16/GAN/index.html">
<meta property="og:site_name" content="桑龙的博客">
<meta property="og:description" content="Generative adversarial network (GAN) has got a lot of great results in many generative tasks to create the real-world objects which is unseen before. Inspired by Generative adversarial network (GAN) h">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://sanglongbest.github.io/2020/01/16/GAN/!--￼0--">
<meta property="og:image" content="http://sanglongbest.github.io/2020/01/16/GAN/!--￼1--">
<meta property="og:image" content="http://sanglongbest.github.io/2020/01/16/GAN/!--￼2--">
<meta property="og:image" content="http://sanglongbest.github.io/2020/01/16/GAN/!--￼3--">
<meta property="og:image" content="http://sanglongbest.github.io/2020/01/16/GAN/!--￼4--">
<meta property="og:image" content="http://sanglongbest.github.io/2020/01/16/GAN/!--￼5--">
<meta property="og:image" content="http://sanglongbest.github.io/2020/01/16/GAN/!--￼6--">
<meta property="og:image" content="http://sanglongbest.github.io/2020/01/16/GAN/!--￼7--">
<meta property="og:image" content="http://sanglongbest.github.io/2020/01/16/GAN/!--￼8--">
<meta property="og:image" content="http://sanglongbest.github.io/2020/01/16/GAN/!--￼9--">
<meta property="og:updated_time" content="2020-02-07T15:07:36.436Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Telling GAN">
<meta name="twitter:description" content="Generative adversarial network (GAN) has got a lot of great results in many generative tasks to create the real-world objects which is unseen before. Inspired by Generative adversarial network (GAN) h">
<meta name="twitter:image" content="http://sanglongbest.github.io/2020/01/16/GAN/!--￼0--">






  <link rel="canonical" href="http://sanglongbest.github.io/2020/01/16/GAN/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Telling GAN | 桑龙的博客</title>
  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119078527-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119078527-1');
</script>






  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">桑龙的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://sanglongbest.github.io/2020/01/16/GAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="桑龙(Yang Liu）">
      <meta itemprop="description" content>
      <meta itemprop="image" content="https://i1.rgstatic.net/ii/profile.image/623958063853569-1525774603825_Q128/Yang_Liu753.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="桑龙的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Telling GAN
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2020-01-16 11:58:28" itemprop="dateCreated datePublished" datetime="2020-01-16T11:58:28+00:00">2020-01-16</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2020-02-07 15:07:36" itemprop="dateModified" datetime="2020-02-07T15:07:36+00:00">2020-02-07</time>
              
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2020/01/16/GAN/#comments" itemprop="discussionUrl">
                
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/01/16/GAN/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Generative adversarial network (GAN) has got a lot of great results in many generative tasks to create the real-world objects which is unseen before. Inspired by</p>
<p>Generative adversarial network (GAN) has shown great results in many generative tasks to replicate the real-world rich content such as images, human language, and music. It is inspired by game theory: two models, a generator and a critic, are competing with each other while making each other stronger at the same time. However, it is rather challenging to train a GAN model, as people are facing issues like training instability or failure to converge.</p>
<p>Here I would like to explain the maths behind the generative adversarial network framework, why it is hard to be trained, and finally introduce a modified version of GAN intended to solve the training difficulties.</p>
<p>{: class="table-of-content"}</p>
<p>TOC {:toc} Kullback–Leibler and Jensen–Shannon Divergence Before we start examining GANs closely, let us first review two metrics for quantifying the similarity between two probability distributions.</p>
<ol type="1">
<li>KL (Kullback–Leibler) divergence measures how one probability distribution <span class="math display">\[p\]</span> diverges from a second expected probability distribution <span class="math display">\[q\]</span>.</li>
</ol>
<p><span class="math display">\[ D_{KL}(p | q) = \int_x p(x) \log \frac{p(x)}{q(x)} dx \]</span></p>
<p><span class="math display">\[D_{KL}\]</span> achieves the minimum zero when <span class="math display">\[p(x)\]</span> == <span class="math display">\[q(x)\]</span> everywhere.</p>
<p>It is noticeable according to the formula that KL divergence is asymmetric. In cases where <span class="math display">\[p(x)\]</span> is close to zero, but <span class="math display">\[q(x)\]</span> is significantly non-zero, the <span class="math display">\[q\]</span>'s effect is disregarded. It could cause buggy results when we just want to measure the similarity between two equally important distributions.</p>
<ol start="2" type="1">
<li>Jensen–Shannon Divergence is another measure of similarity between two probability distributions, bounded by <span class="math display">\[[0, 1]\]</span>. JS divergence is symmetric (yay!) and more smooth. Check this Quora post if you are interested in reading more about the comparison between KL divergence and JS divergence.</li>
</ol>
<p><span class="math display">\[ D_{JS}(p | q) = \frac{1}{2} D_{KL}(p | \frac{p + q}{2}) + \frac{1}{2} D_{KL}(q | \frac{p + q}{2}) \]</span></p>
<p><img src="!--￼0--" alt="KL and JS divergence"> {: style="width: 640px;" class="center"} Fig. 1. Given two Gaussian distribution, <span class="math display">\[p\]</span> with mean=0 and std=1 and <span class="math display">\[q\]</span> with mean=1 and std=1. The average of two distributions is labelled as <span class="math display">\[m=(p+q)/2\]</span>. KL divergence <span class="math display">\[D_{KL}\]</span> is asymmetric but JS divergence <span class="math display">\[D_{JS}\]</span> is symmetric.</p>
<p>Some believe (Huszar, 2015) that one reason behind GANs' big success is switching the loss function from asymmetric KL divergence in traditional maximum-likelihood approach to symmetric JS divergence. We will discuss more on this point in the next section.</p>
<p>Generative Adversarial Network (GAN) GAN consists of two models:</p>
<p>A discriminator <span class="math display">\[D\]</span> estimates the probability of a given sample coming from the real dataset. It works as a critic and is optimized to tell the fake samples from the real ones. A generator <span class="math display">\[G\]</span> outputs synthetic samples given a noise variable input <span class="math display">\[z\]</span> (<span class="math display">\[z\]</span> brings in potential output diversity). It is trained to capture the real data distribution so that its generative samples can be as real as possible, or in other words, can trick the discriminator to offer a high probability. <img src="!--￼1--" alt="Generative adversarial network"> {: style="width: 600px;" class="center"} Fig. 2. Architecture of a generative adversarial network. (Image source: www.kdnuggets.com/2017/01/generative-...-learning.html)</p>
<p>These two models compete against each other during the training process: the generator <span class="math display">\[G\]</span> is trying hard to trick the discriminator, while the critic model <span class="math display">\[D\]</span> is trying hard not to be cheated. This interesting zero-sum game between two models motivates both to improve their functionalities.</p>
<p>Given,</p>
<p>{: class="info"} | Symbol | Meaning | Notes | | ------------- | ------------- | ------------- | | <span class="math display">\[p_{z}\]</span> | Data distribution over noise input <span class="math display">\[z\]</span> | Usually, just uniform. | | <span class="math display">\[p_{g}\]</span> | The generator's distribution over data <span class="math display">\[x\]</span> | | | <span class="math display">\[p_{r}\]</span>| Data distribution over real sample <span class="math display">\[x\]</span> | |</p>
<p>On one hand, we want to make sure the discriminator <span class="math display">\[D\]</span>'s decisions over real data are accurate by maximizing <span class="math display">\[\mathbb{E}{x \sim p{r}(x)} [\log D(x)]\]</span>. Meanwhile, given a fake sample <span class="math display">\[G(z), z \sim p_z(z)\]</span>, the discriminator is expected to output a probability, <span class="math display">\[D(G(z))\]</span>, close to zero by maximizing <span class="math display">\[\mathbb{E}{z \sim p{z}(z)} [\log (1 - D(G(z)))]\]</span>.</p>
<p>On the other hand, the generator is trained to increase the chances of <span class="math display">\[D\]</span> producing a high probability for a fake example, thus to minimize <span class="math display">\[\mathbb{E}{z \sim p{z}(z)} [\log (1 - D(G(z)))]\]</span>.</p>
<p>When combining both aspects together, <span class="math display">\[D\]</span> and <span class="math display">\[G\]</span> are playing a minimax game in which we should optimize the following loss function:</p>
<p><span class="math display">\[ \begin{aligned} \min_G \max_D L(D, G) &amp; = \mathbb{E}{x \sim p{r}(x)} [\log D(x)] + \mathbb{E}{z \sim p_z(z)} [\log(1 - D(G(z)))] \ &amp; = \mathbb{E}{x \sim p_{r}(x)} [\log D(x)] + \mathbb{E}_{x \sim p_g(x)} [\log(1 - D(x)] \end{aligned} \]</span></p>
<p>(<span class="math display">\[\mathbb{E}{x \sim p{r}(x)} [\log D(x)]\]</span> has no impact on <span class="math display">\[G\]</span> during gradient descent updates.)</p>
<p>What is the optimal value for D? Now we have a well-defined loss function. Let's first examine what is the best value for <span class="math display">\[D\]</span>.</p>
<p><span class="math display">\[ L(G, D) = \int_x \bigg( p_{r}(x) \log(D(x)) + p_g (x) \log(1 - D(x)) \bigg) dx \]</span></p>
<p>Since we are interested in what is the best value of <span class="math display">\[D(x)\]</span> to maximize <span class="math display">\[L(G, D)\]</span>, let us label</p>
<p><span class="math display">\[ \tilde{x} = D(x), A=p_{r}(x), B=p_g(x) \]</span></p>
<p>And then what is inside the integral (we can safely ignore the integral because <span class="math display">\[x\]</span> is sampled over all the possible values) is:</p>
$$
<span class="math display">\[\begin{aligned} f(\tilde{x}) &amp; = A log\tilde{x} + B log(1-\tilde{x}) \

\frac{d f(\tilde{x})}{d \tilde{x}} &amp; = A \frac{1}{ln10} \frac{1}{\tilde{x}} - B \frac{1}{ln10} \frac{1}{1 - \tilde{x}} \ &amp; = \frac{1}{ln10} (\frac{A}{\tilde{x}} - \frac{B}{1-\tilde{x}}) \ &amp; = \frac{1}{ln10} \frac{A - (A + B)\tilde{x}}{\tilde{x} (1 - \tilde{x})} \ \end{aligned}\]</span>
<p>$$</p>
<p>Thus, set <span class="math display">\[\frac{d f(\tilde{x})}{d \tilde{x}} = 0\]</span>, we get the best value of the discriminator: <span class="math display">\[D^(x) = \tilde{x}^ = \frac{A}{A + B} = \frac{p_{r}(x)}{p_{r}(x) + p_g(x)} \in [0, 1]\]</span>.</p>
<p>Once the generator is trained to its optimal, <span class="math display">\[p_g\]</span> gets very close to <span class="math display">\[p_{r}\]</span>. When <span class="math display">\[p_g = p_{r}\]</span>, <span class="math display">\[D^*(x)\]</span> becomes <span class="math display">\[1/2\]</span>.</p>
<p>What is the global optimal? When both <span class="math display">\[G\]</span> and <span class="math display">\[D\]</span> are at their optimal values, we have <span class="math display">\[p_g = p_{r}\]</span> and <span class="math display">\[D^*(x) = 1/2\]</span> and the loss function becomes:</p>
<p><span class="math display">\[ \begin{aligned} L(G, D^) &amp;= \int_x \bigg( p_{r}(x) \log(D^(x)) + p_g (x) \log(1 - D^*(x)) \bigg) dx \ &amp;= \log \frac{1}{2} \int_x p_{r}(x) dx + \log \frac{1}{2} \int_x p_g(x) dx \ &amp;= -2\log2 \end{aligned} \]</span></p>
<p>What does the loss function represent? According to the formula listed, JS divergence between <span class="math display">\[p_{r}\]</span> and <span class="math display">\[p_g\]</span> can be computed as:</p>
<p><span class="math display">\[ \begin{aligned} D_{JS}(p_{r} | p_g) =&amp; \frac{1}{2} D_{KL}(p_{r} || \frac{p_{r} + p_g}{2}) + \frac{1}{2} D_{KL}(p_{g} || \frac{p_{r} + p_g}{2}) \ =&amp; \frac{1}{2} \bigg( \log2 + \int_x p_{r}(x) \log \frac{p_{r}(x)}{p_{r} + p_g(x)} dx \bigg) + \&amp; \frac{1}{2} \bigg( \log2 + \int_x p_g(x) \log \frac{p_g(x)}{p_{r} + p_g(x)} dx \bigg) \ =&amp; \frac{1}{2} \bigg( \log4 + L(G, D^*) \bigg) \end{aligned} \]</span></p>
<p>Thus,</p>
<p><span class="math display">\[ L(G, D^*) = 2D_{JS}(p_{r} | p_g) - 2\log2 \]</span></p>
<p>Essentially the loss function of GAN quantifies the similarity between the generative data distribution <span class="math display">\[p_g\]</span> and the real sample distribution <span class="math display">\[p_{r}\]</span> by JS divergence when the discriminator is optimal. The best <span class="math display">\[G^\]</span> that replicates the real data distribution leads to the minimum <span class="math display">\[L(G^, D^*) = -2\log2\]</span> which is aligned with equations above.</p>
<p>Other Variations of GAN: There are many variations of GANs in different contexts or designed for different tasks. For example, for semi-supervised learning, one idea is to update the discriminator to output real class labels, <span class="math display">\[1, \dots, K-1\]</span>, as well as one fake class label <span class="math display">\[K\]</span>. The generator model aims to trick the discriminator to output a classification label smaller than <span class="math display">\[K\]</span>.</p>
<p>Tensorflow Implementation: carpedm20/DCGAN-tensorflow</p>
<p>Problems in GANs Although GAN has shown great success in the realistic image generation, the training is not easy; The process is known to be slow and unstable.</p>
<p>Hard to achieve Nash equilibrium Salimans et al. (2016) discussed the problem with GAN's gradient-descent-based training procedure. Two models are trained simultaneously to find a Nash equilibrium to a two-player non-cooperative game. However, each model updates its cost independently with no respect to another player in the game. Updating the gradient of both models concurrently cannot guarantee a convergence.</p>
<p>Let's check out a simple example to better understand why it is difficult to find a Nash equilibrium in an non-cooperative game. Suppose one player takes control of <span class="math display">\[x\]</span> to minimize <span class="math display">\[f_1(x) = xy\]</span>, while at the same time the other player constantly updates <span class="math display">\[y\]</span> to minimize <span class="math display">\[f_2(y) = -xy\]</span>.</p>
<p>Because <span class="math display">\[\frac{\partial f_1}{\partial x} = y\]</span> and <span class="math display">\[\frac{\partial f_2}{\partial y} = -x\]</span>, we update <span class="math display">\[x\]</span> with <span class="math display">\[x-\eta \cdot y\]</span> and <span class="math display">\[y\]</span> with <span class="math display">\[y+ \eta \cdot x\]</span> simulitanously in one iteration, where <span class="math display">\[\eta\]</span> is the learning rate. Once <span class="math display">\[x\]</span> and <span class="math display">\[y\]</span> have different signs, every following gradient update causes huge oscillation and the instability gets worse in time, as shown in Fig. 3.</p>
<p><img src="!--￼2--" alt="Nash equilibrium example"> {: style="width: 660px;" class="center"} Fig. 3. A simulation of our example for updating <span class="math display">\[x\]</span> to minimize <span class="math display">\[xy\]</span> and updating <span class="math display">\[y\]</span> to minimize <span class="math display">\[-xy\]</span>. The learning rate <span class="math display">\[\eta = 0.1\]</span>. With more iterations, the oscillation grows more and more unstable.</p>
<p>Low dimensional supports {: class="info"} | Term | Explanation | | ------------ | ------------ | | Manifold | A topological space that locally resembles Euclidean space near each point. Precisely, when this Euclidean space is of dimension <span class="math display">\[n\]</span>, the manifold is referred as <span class="math display">\[n\]</span>-manifold. | | Support | A real-valued function <span class="math display">\[f\]</span> is the subset of the domain containing those elements which are not mapped to zero.</p>
<p>Arjovsky and Bottou (2017) discussed the problem of the supports of <span class="math display">\[p_r\]</span> and <span class="math display">\[p_g\]</span> lying on low dimensional manifolds and how it contributes to the instability of GAN training thoroughly in a very theoretical paper "Towards principled methods for training generative adversarial networks".</p>
<p>The dimensions of many real-world datasets, as represented by <span class="math display">\[p_r\]</span>, only appear to be artificially high. They have been found to concentrate in a lower dimensional manifold. This is actually the fundamental assumption for Manifold Learning. Thinking of the real world images, once the theme or the contained object is fixed, the images have a lot of restrictions to follow, i.e., a dog should have two ears and a tail, and a skyscraper should have a straight and tall body, etc. These restrictions keep images aways from the possibility of having a high-dimensional free form.</p>
<p><span class="math display">\[p_g\]</span> lies in a low dimensional manifolds, too. Whenever the generator is asked to a much larger image like 64x64 given a small dimension, such as 100, noise variable input <span class="math display">\[z\]</span>, the distribution of colors over these 4096 pixels has been defined by the small 100-dimension random number vector and can hardly fill up the whole high dimensional space.</p>
<p>Because both <span class="math display">\[p_g\]</span> and <span class="math display">\[p_r\]</span> rest in low dimensional manifolds, they are almost certainly gonna be disjoint (See Fig. 4). When they have disjoint supports, we are always capable of finding a perfect discriminator that separates real and fake samples 100% correctly. Check the paper if you are curious about the proof.</p>
<p><img src="!--￼3--" alt="Low dimensional manifolds in high dimension space"> {: style="width: 640px;" class="center"} Fig. 4. Low dimensional manifolds in high dimension space can hardly have overlaps. (Left) Two lines in a three-dimension space. (Right) Two surfaces in a three-dimension space.</p>
<p>Vanishing gradient When the discriminator is perfect, we are guaranteed with <span class="math display">\[D(x) = 1, \forall x \in p_r\]</span> and <span class="math display">\[D(x) = 0, \forall x \in p_g\]</span>. Therefore the loss function <span class="math display">\[L\]</span> falls to zero and we end up with no gradient to update the loss during learning iterations. Fig. 5 demonstrates an experiment when the discriminator gets better, the gradient vanishes fast.</p>
<p><img src="!--￼4--" alt="Low dimensional manifolds in high dimension space"> {: style="width: 480px;" class="center"} Fig. 5. First, a DCGAN is trained for 1, 10 and 25 epochs. Then, with the generator fixed, a discriminator is trained from scratch and measure the gradients with the original cost function. We see the gradient norms decay quickly (in log scale), in the best case 5 orders of magnitude after 4000 discriminator iterations. (Image source: Arjovsky and Bottou, 2017)</p>
<p>As a result, training a GAN faces a dilemma:</p>
<p>If the discriminator behaves badly, the generator does not have accurate feedback and the loss function cannot represent the reality. If the discriminator does a great job, the gradient of the loss function drops down to close to zero and the learning becomes super slow or even jammed. This dilemma clearly is capable to make the GAN training very tough.</p>
<p>Mode collapse During the training, the generator may collapse to a setting where it always produces same outputs. This is a common failure case for GANs, commonly referred to as Mode Collapse. Even though the generator might be able to trick the corresponding discriminator, it fails to learn to represent the complex real-world data distribution and gets stuck in a small space with extremely low variety.</p>
<p><img src="!--￼5--" alt="Mode collapse in GAN"> {: style="width: 720px;" class="center"} Fig. 6. A DCGAN model is trained with an MLP network with 4 layers, 512 units and ReLU activation function, configured to lack a strong inductive bias for image generation. The results shows a significant degree of mode collapse. (Image source: Arjovsky, Chintala, &amp; Bottou, 2017.)</p>
<p>Lack of a proper evaluation metric Generative adversarial networks are not born with a good objection function that can inform us the training progress. Without a good evaluation metric, it is like working in the dark. No good sign to tell when to stop; No good indicator to compare the performance of multiple models.</p>
<p>Improved GAN Training The following suggestions are proposed to help stabilize and improve the training of GANs.</p>
<p>First five methods are practical techniques to achieve faster convergence of GAN training, proposed in "Improve Techniques for Training GANs". The last two are proposed in "Towards principled methods for training generative adversarial networks" to solve the problem of disjoint distributions.</p>
<ol type="1">
<li>Feature Matching</li>
</ol>
<p>Feature matching suggests to optimize the discriminator to inspect whether the generator's output matches expected statistics of the real samples. In such a scenario, the new loss function is defined as <span class="math display">\[| \mathbb{E}{x \sim p_r} f(x) - \mathbb{E}{z \sim p_z(z)}f(G(z)) |_2^2 \]</span>, where <span class="math display">\[f(x)\]</span> can be any computation of statistics of features, such as mean or median.</p>
<ol start="2" type="1">
<li>Minibatch Discrimination</li>
</ol>
<p>With minibatch discrimination, the discriminator is able to digest the relationship between training data points in one batch, instead of processing each point independently.</p>
<p>In one minibatch, we approximate the closeness between every pair of samples, <span class="math display">\[c(x_i, x_j)\]</span>, and get the overall summary of one data point by summing up how close it is to other samples in the same batch, <span class="math display">\[o(x_i) = \sum_{j} c(x_i, x_j)\]</span>. Then <span class="math display">\[o(x_i)\]</span> is explicitly added to the input of the model.</p>
<ol start="3" type="1">
<li>Historical Averaging</li>
</ol>
<p>For both models, add <span class="math display">\[ | \Theta - \frac{1}{t} \sum_{i=1}^t \Theta_i |^2 \]</span> into the loss function, where <span class="math display">\[\Theta\]</span> is the model parameter and <span class="math display">\[\Theta_i\]</span> is how the parameter is configured at the past training time <span class="math display">\[i\]</span>. This addition piece penalizes the training speed when <span class="math display">\[\Theta\]</span> is changing too dramatically in time.</p>
<ol start="4" type="1">
<li>One-sided Label Smoothing</li>
</ol>
<p>When feeding the discriminator, instead of providing 1 and 0 labels, use soften values such as 0.9 and 0.1. It is shown to reduce the networks' vulnerability.</p>
<ol start="5" type="1">
<li>Virtual Batch Normalization (VBN)</li>
</ol>
<p>Each data sample is normalized based on a fixed batch ("reference batch") of data rather than within its minibatch. The reference batch is chosen once at the beginning and stays the same through the training.</p>
<p>Theano Implementation: openai/improved-gan</p>
<ol start="6" type="1">
<li>Adding Noises.</li>
</ol>
<p>We now know <span class="math display">\[p_r\]</span> and <span class="math display">\[p_g\]</span> are disjoint in a high dimensional space and it causes the problem of vanishing gradient. To artificially "spread out" the distribution and to create higher chances for two probability distributions to have overlaps, one solution is to add continuous noises onto the inputs of the discriminator <span class="math display">\[D\]</span>.</p>
<ol start="7" type="1">
<li>Use Better Metric of Distribution Similarity</li>
</ol>
<p>The loss function of the vanilla GAN measures the JS divergence between the distributions of <span class="math display">\[p_r\]</span> and <span class="math display">\[p_g\]</span>. This metric fails to provide a meaningful value when two distributions are disjoint.</p>
<p>Wasserstein metric is proposed to replace JS divergence because it has a much smoother value space. See more in the next section.</p>
<p>Wasserstein GAN (WGAN) What is Wasserstein distance? Wasserstein Distance is a measure of the distance between two probability distributions. It is also called Earth Mover's distance, short for EM distance, because informally it can be interpreted as the minimum energy cost of moving and transforming a pile of dirt in the shape of one probability distribution to the shape of the other distribution. The cost is quantified by: the amount of dirt moved x the moving distance.</p>
<p>Let us first look at a simple case where the probability domain is discrete. For example, suppose we have two distributions <span class="math display">\[P\]</span> and <span class="math display">\[Q\]</span>, each has four piles of dirt and both have ten shovelfuls of dirt in total. The numbers of shovelfuls in each dirt pile are assigned as follows:</p>
<p><span class="math display">\[ P_1 = 3, P_2 = 2, P_3 = 1, P_4 = 4\ Q_1 = 1, Q_2 = 2, Q_3 = 4, Q_4 = 3 \]</span></p>
<p>In order to change <span class="math display">\[P\]</span> to look like <span class="math display">\[Q\]</span>, as illustrated in Fig. 7, we:</p>
<p>First move 2 shovelfuls from <span class="math display">\[P_1\]</span> to <span class="math display">\[P_2\]</span> =&gt; <span class="math display">\[(P_1, Q_1)\]</span> match up. Then move 2 shovelfuls from <span class="math display">\[P_2\]</span> to <span class="math display">\[P_3\]</span> =&gt; <span class="math display">\[(P_2, Q_2)\]</span> match up. Finally move 1 shovelfuls from <span class="math display">\[Q_3\]</span> to <span class="math display">\[Q_4\]</span> =&gt; <span class="math display">\[(P_3, Q_3)\]</span> and <span class="math display">\[(P_4, Q_4)\]</span> match up. If we label the cost to pay to make <span class="math display">\[P_i\]</span> and <span class="math display">\[Q_i\]</span> match as <span class="math display">\[\delta_i\]</span>, we would have <span class="math display">\[\delta_{i+1} = \delta_i + P_i - Q_i\]</span> and in the example:</p>
<p><span class="math display">\[ \begin{aligned} \delta_0 &amp;= 0\ \delta_1 &amp;= 0 + 3 - 1 = 2\ \delta_2 &amp;= 2 + 2 - 2 = 2\ \delta_3 &amp;= 2 + 1 - 4 = -1\ \delta_4 &amp;= -1 + 4 - 3 = 0 \end{aligned} \]</span></p>
<p>Finally the Earth Mover's distance is <span class="math display">\[W = \sum \vert \delta_i \vert = 5\]</span>.</p>
<p><img src="!--￼6--" alt="EM distance for discrete case"> {: class="center"} Fig. 7. Step-by-step plan of moving dirt between piles in <span class="math display">\[P\]</span> and <span class="math display">\[Q\]</span> to make them match.</p>
<p>When dealing with the continuous probability domain, the distance formula becomes:</p>
<p><span class="math display">\[ W(p_r, p_g) = \inf_{\gamma \sim \Pi(p_r, p_g)} \mathbb{E}_{(x, y) \sim \gamma}[| x-y |] \]</span></p>
<p>In the formula above, <span class="math display">\[\Pi(p_r, p_g)\]</span> is the set of all possible joint probability distributions between <span class="math display">\[p_r\]</span> and <span class="math display">\[p_g\]</span>. One joint distribution <span class="math display">\[\gamma \in \Pi(p_r, p_g)\]</span> describes one dirt transport plan, same as the discrete example above, but in the continuous probability space. Precisely <span class="math display">\[\gamma(x, y)\]</span> states the percentage of dirt should be transported from point <span class="math display">\[x\]</span> to <span class="math display">\[y\]</span> so as to make <span class="math display">\[x\]</span> follows the same probability distribution of <span class="math display">\[y\]</span>. That's why the marginal distribution over <span class="math display">\[x\]</span> adds up to <span class="math display">\[p_g\]</span>, <span class="math display">\[\sum_{x} \gamma(x, y) = p_g(y)\]</span> (Once we finish moving the planned amount of dirt from every possible <span class="math display">\[x\]</span> to the target <span class="math display">\[y\]</span>, we end up with exactly what <span class="math display">\[y\]</span> has according to <span class="math display">\[p_g\]</span>.) and vice versa <span class="math display">\[\sum_{y} \gamma(x, y) = p_r(x)\]</span>.</p>
<p>When treating <span class="math display">\[x\]</span> as the starting point and <span class="math display">\[y\]</span> as the destination, the total amount of dirt moved is <span class="math display">\[\gamma(x, y)\]</span> and the travelling distance is <span class="math display">\[| x-y |\]</span> and thus the cost is <span class="math display">\[\gamma(x, y) \cdot | x-y |\]</span>. The expected cost averaged across all the <span class="math display">\[(x,y)\]</span> pairs can be easily computed as:</p>
<p><span class="math display">\[ \sum_{x, y} \gamma(x, y) | x-y | = \mathbb{E}_{x, y \sim \gamma} | x-y | \]</span></p>
<p>Finally, we take the minimum one among the costs of all dirt moving solutions as the EM distance. In the definition of Wasserstein distance, the <span class="math display">\[\inf\]</span> (infimum, also known as greatest lower bound) indicates that we are only interested in the smallest cost.</p>
<p>Why Wasserstein is better than JS or KL divergence? Even when two distributions are located in lower dimensional manifolds without overlaps, Wasserstein distance can still provide a meaningful and smooth representation of the distance in-between.</p>
<p>The WGAN paper exemplified the idea with a simple example.</p>
<p>Suppose we have two probability distributions, <span class="math display">\[P\]</span> and <span class="math display">\[Q\]</span>:</p>
<p><span class="math display">\[ \forall (x, y) \in P, x = 0 \text{ and } y \sim U(0, 1)\ \forall (x, y) \in Q, x = \theta, 0 \leq \theta \leq 1 \text{ and } y \sim U(0, 1)\ \]</span></p>
<p><img src="!--￼7--" alt="Simple example"> {: style="width: 480px;" class="center"} Fig. 8. There is no overlap between <span class="math display">\[P\]</span> and <span class="math display">\[Q\]</span> when <span class="math display">\[\theta \neq 0\]</span>.</p>
<p>When <span class="math display">\[\theta \neq 0\]</span>:</p>
<p><span class="math display">\[ \begin{aligned} D_{KL}(P | Q) &amp;= \sum_{x=0, y \sim U(0, 1)} 1 \cdot \log\frac{1}{0} = +\infty \ D_{KL}(Q | P) &amp;= \sum_{x=\theta, y \sim U(0, 1)} 1 \cdot \log\frac{1}{0} = +\infty \ D_{JS}(P, Q) &amp;= \frac{1}{2}(\sum_{x=0, y \sim U(0, 1)} 1 \cdot \log\frac{1}{1/2} + \sum_{x=0, y \sim U(0, 1)} 1 \cdot \log\frac{1}{1/2}) = \log 2\ W(P, Q) &amp;= |\theta| \end{aligned} \]</span></p>
<p>But when <span class="math display">\[\theta = 0\]</span>, two distributions are fully overlapped:</p>
<p><span class="math display">\[ \begin{aligned} D_{KL}(P | Q) &amp;= D_{KL}(Q | P) = D_{JS}(P, Q) = 0\ W(P, Q) &amp;= 0 = \lvert \theta \rvert \end{aligned} \]</span></p>
<p><span class="math display">\[D_{KL}\]</span> gives us inifity when two distributions are disjoint. The value of <span class="math display">\[D_{JS}\]</span> has sudden jump, not differentiable at <span class="math display">\[\theta = 0\]</span>. Only Wasserstein metric provides a smooth measure, which is super helpful for a stable learning process using gradient descents.</p>
<p>Use Wasserstein distance as GAN loss function It is intractable to exhaust all the possible joint distributions in <span class="math display">\[\Pi(p_r, p_g)\]</span> to compute <span class="math display">\[\inf_{\gamma \sim \Pi(p_r, p_g)}\]</span>. Thus the authors proposed a smart transformation of the formula based on the Kantorovich-Rubinstein duality to:</p>
<p><span class="math display">\[ W(p_r, p_g) = \frac{1}{K} \sup_{| f |L \leq K} \mathbb{E}{x \sim p_r}[f(x)] - \mathbb{E}_{x \sim p_g}[f(x)] \]</span></p>
<p>where <span class="math display">\[\sup\]</span> (supremum) is the opposite of <span class="math display">\[inf\]</span> (infimum); we want to measure the least upper bound or, in even simpler words, the maximum value.</p>
<p>Lipschitz continuity?</p>
<p>The function <span class="math display">\[f\]</span> in the new form of Wasserstein metric is demanded to satisfy <span class="math display">\[| f |_L \leq K\]</span>, meaning it should be K-Lipschitz continuous.</p>
<p>A real-valued function <span class="math display">\[f: \mathbb{R} \rightarrow \mathbb{R}\]</span> is called <span class="math display">\[K\]</span>-Lipschitz continuous if there exists a real constant <span class="math display">\[K \geq 0\]</span> such that, for all <span class="math display">\[x_1, x_2 \in \mathbb{R}\]</span>,</p>
<p><span class="math display">\[\lvert f(x_1) - f(x_2) \rvert \leq K \lvert x_1 - x_2 \rvert\]</span></p>
<p>Here <span class="math display">\[K\]</span> is known as a Lipschitz constant for function <span class="math display">\[f(.)\]</span>. Functions that are everywhere continuously differentiable is Lipschitz continuous, because the derivative, estimated as <span class="math display">\[\frac{\lvert f(x_1) - f(x_2) \rvert}{\lvert x_1 - x_2 \rvert}\]</span>, has bounds. However, a Lipschitz continuous function may not be everywhere differentiable, such as <span class="math display">\[f(x) = \lvert x \rvert\]</span>.</p>
<p>Explaining how the transformation happens on the Wasserstein distance formula is worthy of a long post by itself, so I skip the details here. If you are interested in how to compute Wasserstein metric using linear programming, or how to transfer Wasserstein metric into its dual form according to the Kantorovich-Rubinstein Duality, read this awesome post.</p>
<p>Suppose this function <span class="math display">\[f\]</span> comes from a family of K-Lipschitz continuous functions, <span class="math display">\[{ f_w }_{w \in W}\]</span>, parameterized by <span class="math display">\[w\]</span>. In the modified Wasserstein-GAN, the "discriminator" model is used to learn <span class="math display">\[w\]</span> to find a good <span class="math display">\[f_w\]</span> and the loss function is configured as measuring the Wasserstein distance between <span class="math display">\[p_r\]</span> and <span class="math display">\[p_g\]</span>.</p>
<p><span class="math display">\[ L(p_r, p_g) = W(p_r, p_g) = \max_{w \in W} \mathbb{E}{x \sim p_r}[f_w(x)] - \mathbb{E}{z \sim p_r(z)}[f_w(g_\theta(z))] \]</span></p>
<p>Thus the "discriminator" is not a direct critic of telling the fake samples apart from the real ones anymore. Instead, it is trained to learn a <span class="math display">\[K\]</span>-Lipschitz continuous function to help compute Wasserstein distance. As the loss function decreases in the training, the Wasserstein distance gets smaller and the generator model's output grows closer to the real data distribution.</p>
<p>One big problem is to maintain the <span class="math display">\[K\]</span>-Lipschitz continuity of <span class="math display">\[f_w\]</span> during the training in order to make everything work out. The paper presents a simple but very practical trick: After every gradient update, clamp the weights <span class="math display">\[w\]</span> to a small window, such as <span class="math display">\[[-0.01, 0.01]\]</span>, resulting in a compact parameter space <span class="math display">\[W\]</span> and thus <span class="math display">\[f_w\]</span> obtains its lower and upper bounds to preserve the Lipschitz continuity.</p>
<p><img src="!--￼8--" alt="Simple example"> {: style="width: 640px;" class="center"} Fig. 9. Algorithm of Wasserstein generative adversarial network. (Image source: Arjovsky, Chintala, &amp; Bottou, 2017.)</p>
<p>Compared to the original GAN algorithm, the WGAN undertakes the following changes:</p>
<p>After every gradient update on the critic function, clamp the weights to a small fixed range, <span class="math display">\[[-c, c]\]</span>. Use a new loss function derived from the Wasserstein distance, no logarithm anymore. The "discriminator" model does not play as a direct critic but a helper for estimating the Wasserstein metric between real and generated data distribution. Empirically the authors recommended RMSProp optimizer on the critic, rather than a momentum based optimizer such as Adam which could cause instability in the model training. I haven't seen clear theoretical explanation on this point through. Sadly, Wasserstein GAN is not perfect. Even the authors of the original WGAN paper mentioned that "Weight clipping is a clearly terrible way to enforce a Lipschitz constraint" (Oops!). WGAN still suffers from unstable training, slow convergence after weight clipping (when clipping window is too large), and vanishing gradients (when clipping window is too small).</p>
<p>Some improvement, precisely replacing weight clipping with gradient penalty, has been discussed in Gulrajani et al. 2017. I will leave this to a future post.</p>
<p>Example: Create New Pokemons! Just for fun, I tried out carpedm20/DCGAN-tensorflow on a tiny dataset, Pokemon sprites. The dataset only has 900-ish pokemon images, including different levels of same pokemon species.</p>
<p>Let's check out what types of new pokemons the model is able to create. Unfortunately due to the tiny training data, the new pokemons only have rough shapes without details. The shapes and colors do look better with more training epoches! Hooray!</p>
<p><img src="!--￼9--" alt="Pokemon GAN"> {: class="center"} Fig. 10. Train carpedm20/DCGAN-tensorflow on a set of Pokemon sprite images. The sample outputs are listed after training epoches = 7, 21, 49.</p>
<p>If you are interested in a commented version of carpedm20/DCGAN-tensorflow and how to modify it to train WGAN and WGAN with gradient penalty, check lilianweng/unified-gan-tensorflow.</p>
<p>Cited as:</p>
<p><span class="citation" data-cites="article">@article</span>{weng2017gan, title = "From GAN to WGAN", author = "Weng, Lilian", journal = "lilianweng.github.io/lil-log", year = "2017", url = "http://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html" } OR</p>
<p><span class="citation" data-cites="misc">@misc</span>{weng2019gan, title={From GAN to WGAN}, author={Lilian Weng}, year={2019}, eprint={1904.08994}, archivePrefix={arXiv}, primaryClass={cs.LG} } If you notice mistakes and errors in this post, don't hesitate to contact me at [lilian dot wengweng at gmail dot com] and I would be super happy to correct them right away!</p>
<p>See you in the next post :D</p>
<p>References [1] Goodfellow, Ian, et al. "Generative adversarial nets." NIPS, 2014.</p>
<p>[2] Tim Salimans, et al. "Improved techniques for training gans." NIPS 2016.</p>
<p>[3] Martin Arjovsky and Léon Bottou. "Towards principled methods for training generative adversarial networks." arXiv preprint arXiv:1701.04862 (2017).</p>
<p>[4] Martin Arjovsky, Soumith Chintala, and Léon Bottou. "Wasserstein GAN." arXiv preprint arXiv:1701.07875 (2017).</p>
<p>[5] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville. Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028 (2017).</p>
<p>[6] Computing the Earth Mover's Distance under Transformations</p>
<p>[7] Wasserstein GAN and the Kantorovich-Rubinstein Duality</p>
<p>[8] zhuanlan.zhihu.com/p/25071913</p>
<p>[9] Ferenc Huszár. "How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary?." arXiv preprint arXiv:1511.05101 (2015).</p>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/GAN-WGAN-WGAN-GP/" rel="tag"># GAN, WGAN, WGAN-GP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/01/07/job_question/" rel="next" title="面试常见问题">
                <i class="fa fa-chevron-left"></i> 面试常见问题
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/02/07/interview_question/" rel="prev" title="Top Interview Question">
                Top Interview Question <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="https://i1.rgstatic.net/ii/profile.image/623958063853569-1525774603825_Q128/Yang_Liu753.jpg" alt="桑龙(Yang Liu）">
            
              <p class="site-author-name" itemprop="name">桑龙(Yang Liu）</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">28</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">32</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/sanglongbest/" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="yangliu@surrey.ac.uk" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/liu-yang-60-72-90" target="_blank" title="Zhihu"><i class="fa fa-fw fa-globe"></i>Zhihu</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.researchgate.net" target="_blank" title="Resaerchgate"><i class="fa fa-fw fa-globe"></i>Resaerchgate</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-Yang Liu"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">桑龙(Yang Liu）</span>

  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Mist</a> v6.3.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  

  
    <script id="dsq-count-scr" src="https://http-sanglongbest-github-io.disqus.com/count.js" async></script>
  

  
    <script type="text/javascript">
      var disqus_config = function () {
        this.page.url = 'http://sanglongbest.github.io/2020/01/16/GAN/';
        this.page.identifier = '2020/01/16/GAN/';
        this.page.title = 'Telling GAN';
        };
      function loadComments () {
        var d = document, s = d.createElement('script');
        s.src = 'https://http-sanglongbest-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      }
      
        loadComments();
      
    </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  





	





  












  





  

  

  

  

  
  

  

  

  

  

  

</body>
</html>
